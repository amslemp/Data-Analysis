# About Me

I started my data journey while still an academic advisor at a community college. In 2016, I noticed large quantities of data on paper sign-in sheets that recorded the foot traffic for the academic advising office. Painstakingly, I transferred tens of thousands of entries into Excel so that they would be accessible and easier to analyze. My initial analysis were what one would expect from a remedial analyst. Very quickly, I was motivated to get better so that my boss would have better analysis to make decisions on. Therefore, I started with a Udemy course on Excel by Kyle Pew. That led to the discovery of Excel dashboards in 2017 where I learned wonderful tools through Leila Gharani, again, through Udemy. I spent thousands of hours practicing and getting better, learning how to organize data so that it could be easily analyzed, hours that were *outside of* my normal work hours, because I was still technically employed as an academic advisor. Eventually, Ian Littlejohn's course started me down the path of learning Power BI. As an educational institution, we have a partnerhsip with Microsoft, which is why I went with Power BI and not Tableau. As my skills grew and more Chairs of departments, Directors, Deans, and VPs became aware of what I was doing with the data and how efficient I was at executing, I needed to automate processes, which is when I learned VBA coding for Excel. 

By my second year of data analysis with the college, even though I was still technically employed as an advisor, I was in meetings with the President, VPs, and Directors presenting data and dashboards, because I am, and was, the only person with this skillset in the college. This work with the Strategic Enrollment Management Team was eventually recognized by the Board of Trustees of the college. Around this time, dissatisfaction arose in me because I saw gaps in my knowledge; I wanted to be better, to know more, to be a data scientist and not just a data analyst. That is when I applied to Georgia Institute of Technology for their M.S. in Analytics (Data Science). 

To my surprise and delight, Georgia Tech accepted me, and I began my masters with them in Fall of 2020. I was surprised because, to me, they were a long shot since they are the third highest ranked engineering university in the country. I desparately wanted to go to Georgia Tech because they had high requirements for math and computer coding taught by some of the most renowned professors in the world. I deliberately chose the most difficult statistical track they had, because I wanted to master advanced statistical methods. The program exceeded my expectations. It was easily the most challenging degree I have ever earned. At times, I found myself manually calculating statistical models rooted in multi-variate calculus and linear algebra (e.g. calculating marginal and posterior distributions in Bayesian statistics; calculating gradient descent and cost function optimization; PCA calculation of eignevalues and eigenvectors; vectors and dot products in SVMs, etc.). 

And throughout my time at Georgia Institute of Technology, my greatest learning occurred by applying my knowledge to the real-world data of the college at which I worked. The college uses my Holt-Winters tripple exponential smoothing model for credit hour and headcount each semester. Prior to my forecasting models, the college often had forecasts that were wildly off, sometimes by as much as 19%. Now, most of my forecasts are within 0.50% of day one numbers, with the best foreast off by only 0.12%. That model has helped to accurately anticipate real credit hour generation, which is the primary driver of renenue for a community college outside of state and federal funding. Other ML models have helped determine what outreach methods were actually increasing enrollment (A/B testing, logistic regression, etc.). Some models determined the effect of marketing campaigns and where they appear. For instance, I demonstrated from a multiple linear regression model that having marketing directly on the home page increased enrollment for a special Saturday enrollment, on average, by 543 credit hours vs just having it on social media sites and not having it on the home page, which for the college is about $72,000 of enrollment. Other models have shown that changing a marketing campaign would cost the college significantly vs keeping it the way it is, which caused the executive leadership team to leave it the same. Finally, the most recent big ML project created a model that accurately predicts which students will fail to retain from Fall to Fall and what variables were most predictive of that that we could use to increase retention.

# Data Analysis

This repository contains the python code for the high level data analysis I have done for VPs, Executives, and Directors that is typically the result of ad hoc requests. 
What I love about these requests is they typically ask me to do things that were not on my radar. Many non-data analysts/scientists have the interesting projects
and proposals for my time. I love tackeling these requests for them and will typically be able to knock most of them out in a matter of minutes to a few hours at the most. 
For non-data people, every request seems cumbersome. I always assure them that it is not. 

For the VP I work with more than anyone, I always tell him, "The answer is always, 'Yes.' It is just a matter of time, expertise, and access to the data." I have another
repository that demonstrates my ML applications, predictive analytics, and forecasting models. 

## Data Collection

The Python code references csv files that I have gathered from a Banner DB, which is a product of Oracle. I write scripts in PL/SQL to pull the data. From there, I move 
over to Python. For ML tasks, I use a combination of R and Python, depnding on the task. Due to FERPA laws, I cannot attach the raw data files for others to run the scripts. 
