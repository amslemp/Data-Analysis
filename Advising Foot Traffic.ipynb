{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advising Foot Traffic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algo works through all the foot traffic data at each of the locations. From this data, we can clearly see how many students are calling, emailing, or walking into the office throughout the year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "#### BEFORE YOU DOWNLOAD THE CSV'S, YOU NEED TO CONVERT ALL OF THE ###\n",
    "#### 'DATE' COLUMNS TO MM/DD/YY. THIS WILL ALLOW THE TO_DATETIME() ###\n",
    "#### METHOD TO BE PROPERLY APPLIED ###################################\n",
    "######################################################################\n",
    "\n",
    "#Import df\n",
    "\n",
    "boa = pd.read_csv(\"BOA Foot Traffic.csv\", encoding = 'windows-1254')\n",
    "boe = pd.read_csv(\"BOE Foot Traffic.csv\", encoding = 'windows-1254')\n",
    "bsc = pd.read_csv(\"BSC Foot Traffic.csv\", encoding = 'windows-1254')\n",
    "bom = pd.read_csv(\"BOM Foot Traffic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_df(df, location):\n",
    "\n",
    "    #Modify headers \n",
    "    cols = [i.upper() for i in list(df.columns)]\n",
    "    cols = [i.replace('\\n', ' ') for i in cols]\n",
    "    df.columns = cols\n",
    "\n",
    "    #Rename 'TIME IN' column\n",
    "    df = df.rename(columns = {'TIME IN' : 'TIME OF CONTACT'})\n",
    "\n",
    "    #Create 'LOCATION' and 'TIME RANGE' columns\n",
    "    df['LOCATION'] = location\n",
    "    df['TIME RANGE'] = ''\n",
    "\n",
    "    #Convert # from float to int\n",
    "    df['#'] = df['#'].astype(int)\n",
    "\n",
    "    #If df does not have 'ATHLETE' already present, insert empty \n",
    "    #'ATHLETE' column\n",
    "    if 'ATHLETE' not in list(df.columns):\n",
    "        df['ATHLETE'] = ''\n",
    "    else:\n",
    "        df\n",
    "\n",
    "    #Reorganize columns\n",
    "    cols = ['DATE', '#', 'NAME', 'TIME OF CONTACT', 'TIME RANGE', 'LOCATION',\n",
    "            'APPT', 'DISTANCE', 'CURRENT STUDENT', 'NEW STUDENT', 'RETURNING STUDENT', \n",
    "            'HIGH SCHOOL', 'WORKFORCE', 'VETERAN', 'ENROLL','ADD/DROP', 'QUESTIONS', \n",
    "            'MAJOR CHANGE', 'DEGREE CHECK', 'SUSPENSION', 'ATHLETE', 'ADVISOR SIGN', 'ADV TIME']\n",
    "    df = df[cols]\n",
    "\n",
    "    #Sort through all rows and eliminate 'NaN'. More of an aesthetic thing for me.\n",
    "\n",
    "    for i in list(df.columns[6:21]):\n",
    "        temp = df[i]\n",
    "        ls = []\n",
    "        for j in list(temp):\n",
    "            if j in ['x', 'X', 'Phone', 'In Person', 'Zoom', 'Email', 'Central Adv']:\n",
    "                ls.append(j)\n",
    "            else:\n",
    "                ls.append('')\n",
    "        df[i] = ls\n",
    "\n",
    "    #Return modified df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Apply the program to each of the sign-in sheets. \n",
    "boa2 = modify_df(boa, \"BOA\")\n",
    "boe2 = modify_df(boe, \"BOE\")\n",
    "bsc2 = modify_df(bsc, \"BSC\")\n",
    "bom2 = modify_df(bom, \"BOM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the sign in sheets into a single list.\n",
    "fin_ls = [boa2, boe2, bsc2, bom2]\n",
    "\n",
    "#Then use the concat() method to combine each of the sign-in sheets to a single dataframe.\n",
    "fin_mashup = pd.concat(fin_ls).reset_index(drop = True)\n",
    "\n",
    "#Convert 'DATE' column to datetime object. If you have errors thrown in reference to this\n",
    "#code, it is because the dates from one of your sites are not formatted correctly. If you wish\n",
    "#you can simply submit \" errors = 'coerce' \" after the fin_mashup['DATE'] in the to_datetime()\n",
    "#method. However, doing so will simply convert the dates Pandas cannot read to 'NaT', which \n",
    "#is unhelpful. Better to go back in and fix the dates. \n",
    "\n",
    "fin_mashup['DATE'] = pd.to_datetime(fin_mashup['DATE'], errors = 'coerce')\n",
    "\n",
    "#Create dictionary for month\n",
    "month_dict = {1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6:'Jun', \\\n",
    "              7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "\n",
    "#Comprehension for Month\n",
    "mon = [month_dict.get(i) for i in list(fin_mashup['DATE'].dt.month)]\n",
    "\n",
    "#Create dictionary for days of the week\n",
    "day_of_week = {0: 'Mon', 1:'Tues', 2:'Wed', 3:'Thur', 4:'Fri', 5:'Sat', 6:'Sun'}\n",
    "\n",
    "#Comprehension for Days\n",
    "dow = [day_of_week.get(i) for i in list(fin_mashup['DATE'].dt.weekday)]\n",
    "\n",
    "#Combine new columns with old fin_mashup df\n",
    "fin_mashup['MONTH'], fin_mashup['DAY'] = mon, dow\n",
    "\n",
    "#Export mashup to csv\n",
    "mashup = fin_mashup.to_csv('mashup.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The program below creates a new dataframe, aggregated by whatever column you insert.\n",
    "def create_dataframe(df, column):\n",
    "    temp = pd.DataFrame(df.groupby(column)[column].count()) \\\n",
    "            .rename(columns = {column:'Count'})\\\n",
    "            .sort_values('Count', ascending = False).reset_index()\n",
    "    temp['% Contribution'] = round(temp['Count'] / temp['Count'].sum() * 100, 2)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number Seen by Each Advisor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataframe(fin_mashup, \"ADVISOR SIGN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance Students Seen and in What Modality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = create_dataframe(fin_mashup, \"DISTANCE\")\n",
    "\n",
    "ls = []\n",
    "\n",
    "for i in list(distance['DISTANCE']):\n",
    "    if i not in ['Phone', 'Email', 'Central Adv']:\n",
    "        ls.append('Appt or In-Person')\n",
    "    else:\n",
    "        ls.append(i)\n",
    "\n",
    "distance['DISTANCE'] = ls\n",
    "\n",
    "distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Appointment Type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appt = create_dataframe(fin_mashup, 'APPT')\n",
    "\n",
    "ap = []\n",
    "\n",
    "for i in list(appt['APPT']):\n",
    "    if i in ['Phone', 'Zoom', 'In Person']:\n",
    "        ap.append(i)\n",
    "    else:\n",
    "        ap.append('Walk In')\n",
    "\n",
    "appt['APPT'] = ap\n",
    "\n",
    "appt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data by Site: How Many Students Assisted by Location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataframe(fin_mashup, \"LOCATION\")\n",
    "\n",
    "mask1 = fin_mashup['LOCATION'].isin(['BSC', 'BOM'])\n",
    "\n",
    "appt_types = fin_mashup[mask1].reset_index()\n",
    "\n",
    "pd.DataFrame(appt_types.groupby('DISTANCE')['index'].count())\\\n",
    "  .reset_index()\\\n",
    "  .rename(columns = {'index':'Num Of Students'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Day of Week Students Arrive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dow = create_dataframe(fin_mashup, 'DAY')\n",
    "day_of_week = {'Mon':0 , 'Tues':1, 'Wed':2, 'Thur':3, 'Fri':4, 'Sat':5, 'Sun':6}\n",
    "filt = [day_of_week.get(i) for i in dow['DAY']]\n",
    "dow['FILTER'] = filt\n",
    "dow = dow.sort_values('FILTER').drop('FILTER', axis = 1).reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The section below is the new code for the new dashboard setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start setting up the dashboard with the new sign-in sheet setup. (1.26.22)\n",
    "\n",
    "dash = pd.read_csv('Advising Foot Traffic Aug 25 2021 - Present.csv')\n",
    "\n",
    "#Change the '#' column to be the range of 1 to the end of the df. \n",
    "dash['#'] = range(1,len(dash)+1)\n",
    "\n",
    "#Edit out all of the NaNs in the df. Again, an ascthetic thing for me.\n",
    "ls = []\n",
    "for i in dash['LOCATION'].unique():\n",
    "    temp = dash[dash['LOCATION'] == i]\n",
    "    ls.append(modify_df(temp, i))\n",
    "\n",
    "dash = pd.concat(ls)\n",
    "\n",
    "#Begin to make the individual id's for each student's sign in.\n",
    "#First we have to create an origin date.\n",
    "dash['ORIGIN DATE'] = '1/1/1900'\n",
    "\n",
    "#Second, we need to convert the 'DATE' and 'ORIGIN DATE' to a datetime object\n",
    "dash['ORIGIN DATE'], dash['DATE'] = pd.to_datetime(dash['ORIGIN DATE']), pd.to_datetime(dash['DATE'])\n",
    "\n",
    "#Now, to create the numerical day representation for the date as counted from 1/1/1900, \n",
    "#we subtract the current date from the origin date.\n",
    "dash['NEW DATE'] = dash['DATE'] - dash['ORIGIN DATE']\n",
    "\n",
    "#To complete our id, we create a comprehension adding the numerical day with the '#' column.\n",
    "#This creates a unique id for every student who has walked into Butler Advising, either in person\n",
    "#or via phone, email, or zoom.\n",
    "ids = [str(dash['NEW DATE'][i].days) + str(dash['#'][i]) for i in range(len(dash['NEW DATE']))]\n",
    "\n",
    "#Create ID column with the new ids and drop the 'NEW DATE' and 'ORIGIN DATE'\n",
    "dash['ID'] = ids\n",
    "dash = dash.drop(['NEW DATE', 'ORIGIN DATE'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, convert 'Time of Contact' to datetime object. There are some debugging errors here.\n",
    "#We need to make sure that all the times are writen as timeframe objects in the csv. A simple\n",
    "#table creation in csv, filter, scroll to the bottom and any unique methods of entering times\n",
    "#will be revealed. Those all need to be identified and fixed. Where there are empty rows, \n",
    "#typically this is where an advisor was entering a group enrollment and just didn't want to \n",
    "#enter a bunch of times. In such cases, I copy the time closest to it and run with that.\n",
    "#usually somewhere in the neighborhood of 40 students. \n",
    "\n",
    "dash['TIME OF CONTACT'] = pd.to_datetime(dash['TIME OF CONTACT'])\n",
    "\n",
    "dash = dash[dash['TIME OF CONTACT'].isnull() == False].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert 'TIME RANGE' column\n",
    "\n",
    "times = []\n",
    "for i in dash['TIME OF CONTACT']:\n",
    "    times.append(str(i.hour) + ':00-' + str(i.hour) + ':59')\n",
    "    \n",
    "#Add the times object as the column of 'TIME RANGE'. Now I have the complete dataset that I can\n",
    "#start altering into a usable dataset for a dashboard.\n",
    "\n",
    "dash['TIME RANGE'] = times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below are the final steps to convert the data to be ready to convert to dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the data to convert to a dashboard\n",
    "\n",
    "def dashboard_setup(data, columns, col_name):\n",
    "    #Sort through the columns, identifying the index and replacing\n",
    "    #the 'x'|'X' with column name. This for loop loops through each\n",
    "    #individual column, then through each individual row of each \n",
    "    #column. The enumerate() creates a Series with each column that \n",
    "    #has an index and then the value. We convert the dict created with\n",
    "    #each column to a data frame and then stack those data frames into\n",
    "    #a list. \n",
    "    ls = []\n",
    "    for i in columns:\n",
    "        temp = data[i]\n",
    "        ls2 = {}\n",
    "        for j, k in enumerate(temp):\n",
    "            if k == 'x' or k == 'X':\n",
    "                ls2[j] = i\n",
    "        df = pd.DataFrame.from_dict(ls2, orient = 'index')\n",
    "        ls.append(df)\n",
    "        \n",
    "    #Now we use the pd.concat() to convert the list to a data frame. Drop\n",
    "    #duplicates because some rows have multiple 'X's in them, which creates\n",
    "    #two entries at the same location.\n",
    "    df2 = pd.concat(ls).reset_index().rename(columns = {0: col_name})\\\n",
    "                       .sort_values('index')\n",
    "    df2 = df2.drop_duplicates('index')\n",
    "    \n",
    "    #Now we need to identify and store all of the indeces that do not \n",
    "    #appear in the data frame created with df2. \n",
    "    missing = {}\n",
    "    for i in list(data.index):\n",
    "        if i not in list(df2['index']):\n",
    "            missing[i] = ''\n",
    "    \n",
    "    #Convert the missing indeces to a dataframe that has the same\n",
    "    #columns as df2.\n",
    "    missing = pd.DataFrame.from_dict(missing, orient = 'index')\\\n",
    "                .rename(columns = {0: col_name}).reset_index()\n",
    "    \n",
    "    #Append the two data frames together, sort the values from \n",
    "    #lowest to highest by the 'index' column (not the .index())\n",
    "    #then drop the 'index' column and reset_index().\n",
    "    final = df2.append(missing).sort_values('index')\\\n",
    "               .drop('index', axis = 1).reset_index(drop = True)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup 'Type of Student' column\n",
    "stype = dashboard_setup(dash, ['CURRENT STUDENT', 'NEW STUDENT',\n",
    "                               'RETURNING STUDENT', \n",
    "                               'WORKFORCE', 'VETERAN', 'ATHLETE'], 'Type of Student')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup 'Reason for Visit' column\n",
    "reason = dashboard_setup(dash, ['ENROLL', 'ADD/DROP', 'QUESTIONS', \n",
    "                                'MAJOR CHANGE', 'DEGREE CHECK', \n",
    "                                'SUSPENSION'], 'Reason for Visit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add new columns to the dataframe\n",
    "dash['TYPE OF STUDENT'], dash['REASON FOR VISIT'] = stype, reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Alter HIGH SCHOOL column x's to 'HIGH SCHOOL' because the program we \n",
    "#wrote above was creating duplicates. This is because the student workers,\n",
    "#advising staff, and front office manager vary on when they mark a student \n",
    "#only as high school and mark a student as \"current\" or \"new\" *and* \n",
    "#high school. This was causing the original program to create duplicate \n",
    "#indeces. So in order to track both 'current' and 'high school' attributes,\n",
    "#I created this column.\n",
    "\n",
    "dash['MOD HIGH SCHOOL'] =  ['HIGH SCHOOL' if i == 'x' or i == 'X' else '' for i in dash['HIGH SCHOOL']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just as we did above for the mashup, we need to create columns for month\n",
    "#and day. Below is that process.\n",
    "\n",
    "month_dict = {1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6:'Jun', \\\n",
    "              7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "\n",
    "#Comprehension for Month\n",
    "mon = [month_dict.get(i) for i in list(dash['DATE'].dt.month)]\n",
    "\n",
    "#Create dictionary for days of the week\n",
    "day_of_week = {0: 'Mon', 1:'Tues', 2:'Wed', 3:'Thur', 4:'Fri', 5:'Sat', 6:'Sun'}\n",
    "\n",
    "#Comprehension for Days\n",
    "dow = [day_of_week.get(i) for i in list(dash['DATE'].dt.weekday)]\n",
    "\n",
    "#Combine new columns with old dash df\n",
    "dash['MONTH'], dash['DAY'] = mon, dow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture --no-display \n",
    "\n",
    "final_dash = dash[[ 'ID', 'DATE', 'NAME', 'TIME RANGE', 'MONTH', 'DAY', 'LOCATION',\n",
    "                    'APPT', 'DISTANCE', 'TYPE OF STUDENT', 'REASON FOR VISIT', \n",
    "                    'MOD HIGH SCHOOL', 'ADVISOR SIGN', 'ADV TIME']]\n",
    "\n",
    "#The final setup was to create a column that helps us filter advisors by \n",
    "#the employment type (i.e. 'EMP TYPE')\n",
    "sign = {'KL':'28hr', 'KZ':'28hr', 'CJ':'40hr', 'SLB':'28hr','SV':'28hr',\n",
    "         'BG':'40hr', 'AKP':'28hr', 'JW':'40hr', 'AS':'40hr', 'SP':'40hr',\n",
    "         'SB':'20hr', 'DS':'Boss', 'AP':'40hr', 'SS': 'Not Sure', 'MZ':'Adm',\n",
    "         'JD':'Adm', 'MP':'Adm', 'BM':'40hr', 'CS':'Not Sure', 'RM':'40hr',\n",
    "         'SH':'40hr', 'KLA':'28hr', 'GR':'Adj', 'DR':'40hr', 'TB':'28hr', \n",
    "         'JEC':'28hr', 'KB':'28hr', 'KA':'28hr', '':'N/A'}\n",
    "\n",
    "#Since there are times the cell is left blank, pandas records that as a float\n",
    "#variable, which throws and error when doing a comprehension. Therefore, the \n",
    "#code below deals with that problem by filling all of the NaNs with a blank\n",
    "#string.\n",
    "final_dash['ADVISOR SIGN'] = final_dash['ADVISOR SIGN'].fillna('')\n",
    "\n",
    "s = [i.upper() for i in final_dash['ADVISOR SIGN']]\n",
    "\n",
    "final_dash['ADVISOR SIGN'] = s\n",
    "\n",
    "#Link dictionary we created to the advisor signatures and create new \n",
    "#columns with this new attribute\n",
    "s2 = [sign.get(i) for i in final_dash['ADVISOR SIGN']]\n",
    "\n",
    "final_dash['Emp Type'] = s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export dashbaord to csv\n",
    "dashboard = final_dash.to_csv('Dashboard Setup.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is the code for the old dashboard setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Students per month\n",
    "\n",
    "month = create_dataframe(df, 'Month')\n",
    "num = [4, 8, 12, 2, 1, 7, 6, 3, 5, 11, 10, 9]\n",
    "month['num'] = num\n",
    "month = month.sort_values('num', axis = 0).reset_index(drop = True)[['Month', 'Count', '% Contribution']]\n",
    "\n",
    "#Students seen per week over the last year, sorted from WK 1 to WK 4b. New year is indicated with the week \n",
    "#number followed by the letter 'b'.\n",
    "\n",
    "wks = pd.DataFrame(df.groupby('Week of Enrollment')['Week of Enrollment'].count()).rename(columns = {'Week of Enrollment':'Num Per Week'}).reset_index()\n",
    "wks['Week Num'] = wks['Week of Enrollment'].str.split(\" \").str[1]\n",
    "a = [0, 9, 10, 11, 12,13, 14, 15, 16, 17, 18, 52, 1, 19, 20, 21, 22, 23, 24, 25, \\\n",
    "     26, 27, 28, 53, 2, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 54, 3, 39, 40, 41, 42, \\\n",
    "     43, 44, 45, 46, 47, 48, 55, 4, 49, 50, 51, 5, 6, 7, 8]\n",
    "wks['num'] = a\n",
    "wks = wks.sort_values('num', axis = 0).reset_index(drop = True)\n",
    "wks = wks[['Week of Enrollment', 'Num Per Week']]\n",
    "\n",
    "#Day of Week \n",
    "\n",
    "dofw = create_dataframe(df, 'Day of WK')\n",
    "num = [5, 1, 4, 6, 7, 2, 3]\n",
    "dofw['num'] = num\n",
    "dofw = dofw.sort_values('num', axis = 0).reset_index(drop = True)[['Day of WK', 'Count', '% Contribution']]\n",
    "\n",
    "#Students Seen by Time of Day\n",
    "\n",
    "tm = create_dataframe(df, 'Time Range')\n",
    "num = [3, 4, 5, 6, 7, 8, 9, 10, 11, 1, 2]\n",
    "tm['num'] = num\n",
    "tm = tm.sort_values('num', axis = 0).reset_index(drop = True)\n",
    "tm = tm[['Time Range', 'Count', '% Contribution']]\n",
    "\n",
    "#Student assisted by method of contact.\n",
    "\n",
    "meth_of_cont = create_dataframe(df, 'Contact Method')\n",
    "\n",
    "#Reason for visit\n",
    "\n",
    "reason = create_dataframe(df, 'Reason for Visit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function requires that the first column is the \"Location,\" then it is set up to create a pivot \n",
    "#table that has multiple other potential columns under \"columns\" parameter. Or maybe another way to \n",
    "#look at it is that columns[0] is set to be the index of the pivot table, no matter how you look at \n",
    "#it.\n",
    "\n",
    "def loc_and_col(df, columns):\n",
    "    assert isinstance(columns, list)\n",
    "    newdf = pd.DataFrame(df.groupby(columns).size()) \\\n",
    "                        .rename(columns = {0: 'Num of Students'}).reset_index()\n",
    "    newdf = newdf.pivot_table(values = 'Num of Students', index = columns[0], columns = columns[1:]).fillna('')\n",
    "    return newdf\n",
    "\n",
    "by_month = loc_and_col(df, columns = ['Location', 'Month'])\n",
    "\n",
    "by_reason = loc_and_col(df, columns = ['Location', 'Reason for Visit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dashboard Setup\n",
    "\n",
    "The code below is for setting up the data for the dashboard. It has been cleaned, organized, and programmed so that a usable dataframe is put out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc2 = setup_bsc(bsc)\n",
    "boe2 = setup_boe(boe)\n",
    "boa2 = setup_boa(boa)\n",
    "bom2 = setup_bom(bom)\n",
    "dist2 = setup_dist(dist)\n",
    "\n",
    "ls_fin = [boa2, boe2, bsc2, bom2, dist2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, one of the things you need to make sure of is that all of the dates are \n",
    "#formatted into the '%m/%d' format.  That way, the code below works. So this process\n",
    "#happens after you run the code above, open up the 'mashup.csv' and format the date,\n",
    "#and resave. Then run the code below.\n",
    "\n",
    "rev_mash = pd.read_csv('mashup.csv')\n",
    "\n",
    "#In order to create the ids I've been using for a few years now, in python, it is a\n",
    "#little more convoluted (for me at least as I don't know a more streamlined way \n",
    "#to execute it yet). \n",
    "\n",
    "#First, we have to create a column with the origin date, and then convert it to a \n",
    "#datetime object. \n",
    "\n",
    "rev_mash['ORIGIN_DATE'] = '1/1/1900'\n",
    "rev_mash['ORIGIN_DATE'] = pd.to_datetime(rev_mash['ORIGIN_DATE'])\n",
    "\n",
    "#Then, create a list of the locations that we will use to cycle though\n",
    "#for our for loop.\n",
    "\n",
    "loc = list(rev_mash['LOCATION'].unique())\n",
    "\n",
    "#This for loop first creates a temporary df filtered by the loction.\n",
    "#Then, for each temporary dataframe, we have to convert the 'DATE' \n",
    "#column into a datetime object and notify pandas it is in the format \n",
    "#of '%m/%d'.\n",
    "#Then we create a list of the 'DATE' column from the temporary df and \n",
    "#cycle through those dates using a for loop. This for loop with go through\n",
    "#each date and change the default year, which is 1900, to 2021.  \n",
    "#Finally, we insert the converted dates as the new 'DATE' column and save that\n",
    "#to 'ls'. That will create five lists of dfs, each corresponding to one of the \n",
    "#locations we used to filter each temporary df. \n",
    "\n",
    "ls = []\n",
    "for i in loc:  \n",
    "    temp = rev_mash[rev_mash['LOCATION'] == i]\n",
    "    temp['DATE'] = pd.to_datetime(temp['DATE'], format='%m/%d')\n",
    "    dates = list(temp['DATE'])\n",
    "    ls2 = []\n",
    "    for j in dates:\n",
    "        ls2.append(j.replace(year = 2021))\n",
    "    temp['DATE'] = ls2\n",
    "    ls.append(temp)\n",
    "\n",
    "#With the list of dfs, we use the pd.concat() to draw them all back together.\n",
    "\n",
    "newdf = pd.concat(ls)\n",
    "\n",
    "#Now, we are set up to subtract the 2021 date in the 'DATE' column from the \n",
    "#1/1/1900 date in the 'ORIGIN_DATE' column.\n",
    "\n",
    "newdf['NEW_DATE'] = newdf['DATE'] - newdf['ORIGIN_DATE']\n",
    "\n",
    "#Now, as part of the ids I've created over the last few years, I needed to change\n",
    "#the '#' to a list of continuous integers.\n",
    "\n",
    "newdf['#'] = list(range(1, len(newdf)+1))\n",
    "\n",
    "#Now for the final touch. We create the id. The unique id for every student\n",
    "#who 'walked' into Butler either virtually or phyically.\n",
    "#This is done by accessing the days attribute from the *timedelta* object\n",
    "#created by the 'NEW_DATE' column, converting it to a string variable,\n",
    "#and concatenating it with the number in the corresponding row from the '#'\n",
    "#column. If you do not convert these to strings, it will just add the two \n",
    "#integers. \n",
    "\n",
    "ids = [str(newdf['NEW_DATE'][i].days) + str(newdf['#'][i]) for i in range(len(newdf['NEW_DATE']))]\n",
    "newdf['ID'] = ids\n",
    "newdf = newdf.drop(['ORIGIN_DATE', 'NEW_DATE'], axis = 1)\n",
    "\n",
    "#Now, we create ranges for the 'TIME RANGE' column.\n",
    "\n",
    "newdf['TIME OF CONTACT'] = pd.to_datetime(newdf['TIME OF CONTACT'])\n",
    "\n",
    "newdf = newdf[newdf['TIME OF CONTACT'].isnull() == False].reset_index(drop = True)\n",
    "\n",
    "#If for some reason I ever want to figure out how to take a datetime object and\n",
    "#isolate the hour and minute and format it into a string '%h:%m', below is how I \n",
    "#figured out how to do it before realizing it was a waste of my time for \n",
    "#what I needed to do for this dataset.\n",
    "\n",
    "#times = {}\n",
    "#for i, j in enumerate(newdf['TIME OF CONTACT']):\n",
    "#    times[i] = j.time()\n",
    "\n",
    "#times2 = []\n",
    "#for i in range(len(times)):\n",
    "#    if list(times.values())[i].minute < 10:\n",
    "#        times2.append(str(list(times.values())[i].hour) + \":\" + '0' + str(list(times.values())[i].minute))\n",
    "#    else:\n",
    "#        times2.append(str(list(times.values())[i].hour) + \":\" + str(list(times.values())[i].minute))\n",
    "\n",
    "#Below is the way to create the time ranges. Very simple.\n",
    "\n",
    "times = []\n",
    "for i in newdf['TIME OF CONTACT']:\n",
    "    if i.hour == 8:\n",
    "        times.append('8:00-8:59')\n",
    "    elif i.hour == 9:\n",
    "        times.append('9:00-9:59')\n",
    "    elif i.hour == 10:\n",
    "        times.append('10:00-10:59')\n",
    "    elif i.hour == 11:\n",
    "        times.append('11:00-11:59')\n",
    "    elif i.hour == 12:\n",
    "        times.append('12:00-12:59')\n",
    "    elif i.hour == 13:\n",
    "        times.append('1:00-1:59')\n",
    "    elif i.hour == 14:\n",
    "        times.append('2:00-2:59')\n",
    "    elif i.hour == 15:\n",
    "        times.append('3:00-3:59')\n",
    "    elif i.hour == 16:\n",
    "        times.append('4:00-4:59')\n",
    "    elif i.hour == 17:\n",
    "        times.append('5:00-5:59')\n",
    "    elif i.hour == 18:\n",
    "        times.append('6:00-6:59')\n",
    "    elif i.hour == 19:\n",
    "        times.append('7:00-7:59')\n",
    "    elif i.hour == 1:\n",
    "        times.append('1:00-1:59')\n",
    "    elif i.hour == 2:\n",
    "        times.append('2:00-2:59')\n",
    "    elif i.hour == 3:\n",
    "        times.append('3:00-3:59')\n",
    "    elif i.hour == 4:\n",
    "        times.append('4:00-4:59')\n",
    "    elif i.hour == 5:\n",
    "        times.append('5:00-5:59')\n",
    "    elif i.hour == 6:\n",
    "        times.append('6:00-6:59')\n",
    "    elif i.hour == 7:\n",
    "        times.append('7:00-7:59')\n",
    "\n",
    "#Add the times object as the column of 'TIME RANGE'. Now I have the complete dataset that I can\n",
    "#start altering into a usable dataset for a dashboard.\n",
    "\n",
    "newdf['TIME RANGE'] = times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = newdf[['ID', 'DATE', '#', 'NAME', 'TIME OF CONTACT', 'TIME RANGE', 'LOCATION',\n",
    "               'INTEROFFICE', 'INSTITUTIONAL', 'COMMITTEE', 'PHONE', 'EMAIL', 'TEXT',\n",
    "               'VIRTUAL F2F', 'REN', 'GUEST STUDENT', 'ADVISING@BUTLERCC', 'AVISO',\n",
    "               'OTHER', 'CONTINUING', 'NEW STUDENT', 'VETERAN', 'HIGH SCHOOL', 'SUSPENSION', \n",
    "               'WIA/TAA', 'ENROLLMENT', 'DEGREE CHECK', 'ADD/DROP', 'QUESTIONS', \n",
    "               'MAJOR CHANGE', 'FOLLOW UP', 'APPOINTMENT', 'ATHLETE',\"INT'L\", \n",
    "               'PAYMENT', 'BASE PASS', 'ADVISOR SIGN', 'ADV TIME']]\n",
    "\n",
    "#Create function that will allow us to quickly go through each column\n",
    "#and consolidate them into aggregated groups.\n",
    "\n",
    "def dashboard_setup(df, columns, col_name):\n",
    "    ls = []\n",
    "    for j in columns:\n",
    "        temp = df[j]\n",
    "        ls2 = []\n",
    "        for i in temp:\n",
    "            if i == 'x' or i == 'X':\n",
    "                ls2.append(i)\n",
    "            else:\n",
    "                ls2.append('')\n",
    "        df2 = pd.DataFrame(pd.Series(ls2))\n",
    "        df2 = pd.DataFrame(list(zip(list(df2[df2[0].isin(['x', 'X'])].index), \\\n",
    "                 [j] * len(df2))))\n",
    "        ls.append(df2)\n",
    "\n",
    "    dash_df = pd.concat(ls).sort_values(0).reset_index(drop = True)\n",
    "\n",
    "    missing = [i for i in range(len(newdf)) if i not in list(dash_df[0])]\n",
    "    \n",
    "    miss_df = pd.DataFrame(missing)\n",
    "    miss_df[1] = ''\n",
    "    final = dash_df.append(miss_df).drop_duplicates(0) \\\n",
    "                   .sort_values(0).reset_index(drop = True)\n",
    "    final[col_name] = final[1]\n",
    "    final = final.drop([0, 1], axis = 1)\n",
    "    return final\n",
    "\n",
    "#Use function we just created to consolidate each of the columns into \n",
    "#single columns.\n",
    "\n",
    "contact = dashboard_setup(newdf, ['PHONE', 'EMAIL', 'TEXT', 'VIRTUAL F2F'], 'CONTACT METHOD')\n",
    "bu_contacts = dashboard_setup(newdf, ['INTEROFFICE', 'INSTITUTIONAL', 'COMMITTEE'], 'BUTLER CONTACTS')\n",
    "source = dashboard_setup(newdf, ['REN', 'GUEST STUDENT', 'ADVISING@BUTLERCC', 'AVISO','OTHER'], 'SOURCE')\n",
    "stype = dashboard_setup(newdf, ['CONTINUING', 'NEW STUDENT', 'VETERAN', 'HIGH SCHOOL', 'SUSPENSION', 'WIA/TAA', 'ATHLETE', \"INT'L\"], 'TYPE OF STUDENT')\n",
    "reason = dashboard_setup(newdf, ['ENROLLMENT', 'DEGREE CHECK', 'ADD/DROP', 'QUESTIONS', 'MAJOR CHANGE', 'PAYMENT', 'BASE PASS'], 'REASON FOR VISIT')\n",
    "\n",
    "#Join the some of the columns from the 'newdf' with a new df, 'dash_df'.\n",
    "\n",
    "dash_df = newdf[['ID', 'DATE', 'NAME', 'TIME RANGE', 'LOCATION', 'ADVISOR SIGN']]\n",
    "dash_df['CONTACT METHOD'], dash_df['BUTLER CONTACTS'], dash_df['SOURCE'], dash_df['TYPE OF STUDENT'], dash_df['REASON FOR VISIT'] = contact, bu_contacts, source, stype, reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull year and month from datetime object.\n",
    "\n",
    "dash_df['YEAR'] = dash_df['DATE'].dt.year\n",
    "dash_df['MONTH'] = dash_df['DATE'].dt.month_name().str.slice(stop = 3)\n",
    "\n",
    "#Create dictionary for day of week and implement.\n",
    "\n",
    "dofw = {0:'M', 1:'T', 2:'W', 3:'R', 4:'F', 5:'S', 6:'SU'}\n",
    "dash_df['DAY OF WK'] = dash_df['DATE'].dt.dayofweek\n",
    "days = [dofw.get(i) for i in dash_df['DAY OF WK']]\n",
    "dash_df['DAY OF WK'] = days\n",
    "\n",
    "#Craete week of enrollment column. I am considering changing this from\n",
    "#what I've been doing for the last year or so to something that aligns with \n",
    "#industry standards.\n",
    "\n",
    "dash_df['WEEK OF ENROLLMENT'] = dash_df['DATE'].dt.isocalendar().week\n",
    "\n",
    "#Final column creation. Create a modified column from the 'Butler Contact'\n",
    "#column so that we can easily filter by 'Butler Contact' or 'Student Contact.'\n",
    "\n",
    "butler_con_mod = []\n",
    "\n",
    "for i in dash_df['BUTLER CONTACTS']:\n",
    "    if i == '':\n",
    "        butler_con_mod.append('Student Contact')\n",
    "    else:\n",
    "        butler_con_mod.append('Butler Contact')\n",
    "\n",
    "dash_df['BUTLER CONTACTS MOD'] = butler_con_mod\n",
    "\n",
    "#Final dash_df sorted.\n",
    "\n",
    "dash_df = dash_df[['ID', 'DATE', 'YEAR', 'MONTH', 'DAY OF WK', \n",
    "                   'WEEK OF ENROLLMENT', 'NAME', 'TIME RANGE', 'LOCATION',\n",
    "                   'CONTACT METHOD', 'BUTLER CONTACTS', 'BUTLER CONTACTS MOD', \n",
    "                   'SOURCE', 'TYPE OF STUDENT', 'REASON FOR VISIT', \n",
    "                   'ADVISOR SIGN']]\n",
    "\n",
    "dash_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
